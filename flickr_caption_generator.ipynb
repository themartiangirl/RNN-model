{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from queue import PriorityQueue\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, RepeatVector, Concatenate, Activation\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_list(filename):\n",
    "    with open(filename,'r') as image_list_f: \n",
    "        return [line.strip() for line in image_list_f]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = load_image_list(os.path.join(FLICKR_PATH, 'Flickr_8k.trainImages.txt'))\n",
    "dev_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.devImages.txt'))\n",
    "test_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.testImages.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_list), len(dev_list), len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = os.path.join(FLICKR_PATH, \"Flickr8k_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(os.path.join(IMG_PATH, dev_list[20]))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cant't see the image, try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "299x299 pixels, with 3 colours channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = np.asarray(image.resize((299,299))) / 255.0\n",
    "plt.imshow(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_name):\n",
    "    image = PIL.Image.open(os.path.join(IMG_PATH, image_name))\n",
    "    return np.asarray(image.resize((299,299))) / 255.0                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(dev_list[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "img_model = InceptionV3(weights='imagenet') # This will download the weight files for you and might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model.summary() # this is quite a complex model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a prediction model,so the output is typically a softmax-activated vector representing 1000 possible object types. Because we are interested in an encoded representation of the image we are just going to use the second-to-last layer as a source of image encodings. Each image will be encoded as a vector of size 2048. \n",
    "\n",
    "Please use the following hack: hook up the input into a new Keras model and use the penultimate layer of the existing model as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = img_model.input\n",
    "new_output = img_model.layers[-2].output\n",
    "img_encoder = Model(new_input, new_output) # This is the final Keras image encoder model we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you may want to add a GPU to the VM you are using (if not using already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_image = img_encoder.predict(np.array([new_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_model = InceptionV3(weights='imagenet')\n",
    "def img_generator(img_list):\n",
    "    for image in img_list:\n",
    "        image_loaded = get_image(image)\n",
    "        yield np.array([image_loaded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = img_encoder.predict_generator(img_generator(train_list), steps=len(train_list), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dev = img_encoder.predict_generator(img_generator(dev_list), steps=len(dev_list), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_test = img_encoder.predict_generator(img_generator(test_list), steps=len(test_list), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a suitable location here\n",
    "OUTPUT_PATH = \"suitablelocation\" \n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_train.npy\"), enc_train)\n",
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_dev.npy\"), enc_dev)\n",
    "np.save(os.path.join(OUTPUT_PATH,\"encoded_images_test.npy\"), enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_descriptions(filename):    \n",
    "    image_descriptions = defaultdict(list)\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            tabsplit = line.split('\\t')\n",
    "            name = tabsplit[0].split('#')[0]\n",
    "            description = [\"<START>\"] + tabsplit[1].lower().split() + [\"<END>\"]\n",
    "            image_descriptions[name].append(description)\n",
    "    return image_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = read_image_descriptions(f\"{FLICKR_PATH}/Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(descriptions[dev_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_to_word(descriptions):\n",
    "    words = set()\n",
    "    for doc in descriptions.values():\n",
    "        for description in doc:\n",
    "            for word in description:\n",
    "                words.add(word)\n",
    "    return {ix:word for ix, word in enumerate(sorted(words))}\n",
    "\n",
    "def get_word_to_id(descriptions):\n",
    "    words = set()\n",
    "    for doc in descriptions.values():\n",
    "        for description in doc:\n",
    "            for word in description:\n",
    "                words.add(word)\n",
    "    return {word:ix for ix, word in enumerate(sorted(words))}\n",
    "\n",
    "id_to_word = get_id_to_word(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = get_word_to_id(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(description) for image_id in train_list for description in descriptions[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM=300\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "# Text input\n",
    "text_input = Input(shape=(MAX_LEN,))\n",
    "embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\n",
    "x = Bidirectional(LSTM(512, return_sequences=False))(embedding)\n",
    "pred = Dense(vocab_size, activation='softmax')(x)\n",
    "model = Model(inputs=[text_input],outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_training_generator(batch_size=128):\n",
    "    while True:\n",
    "        batch_image_ids = np.random.choice(train_list, batch_size)\n",
    "        \n",
    "        input_sequences_list = []\n",
    "        output_words_list = []\n",
    "\n",
    "        for image_id in batch_image_ids:\n",
    "            descs = descriptions[image_id]\n",
    "            desc = random.choice(descs)\n",
    "            tokenized_desc = [word_to_id.get(word, None) for word in desc]\n",
    "\n",
    "            tokenized_desc = [word_id for word_id in tokenized_desc if word_id is not None]\n",
    "\n",
    "            input_sequences = []\n",
    "            output_words = []\n",
    "\n",
    "            for i in range(1, len(tokenized_desc)):\n",
    "                input_seq = tokenized_desc[:i]\n",
    "                output_word = tokenized_desc[i]\n",
    "\n",
    "                input_sequences.append(input_seq)\n",
    "                output_words.append(output_word)\n",
    "\n",
    "            input_sequences = pad_sequences(input_sequences, maxlen=MAX_LEN, padding='pre')\n",
    "            output_words = to_categorical(output_words, num_classes=vocab_size)\n",
    "            input_sequences_list.append(input_sequences)\n",
    "            output_words_list.append(output_words)\n",
    "\n",
    "\n",
    "        input_batch = np.vstack(input_sequences_list)\n",
    "        output_batch = np.vstack(output_words_list)\n",
    "\n",
    "        yield (input_batch, output_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finallyyy, train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "generator = text_training_generator(batch_size)\n",
    "steps = len(train_list) * MAX_LEN // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder():\n",
    "    input_seq = [word_to_id['<START>']]\n",
    "\n",
    "    # Loop until the <END> token is predicted or the sequence reaches MAX_LEN words\n",
    "    while len(input_seq) < MAX_LEN:\n",
    "        padded_input_seq = pad_sequences([input_seq], maxlen=MAX_LEN, padding='pre')\n",
    "        predicted_word_probs = model.predict(padded_input_seq)\n",
    "        most_likely_word_id = np.argmax(predicted_word_probs, axis=-1)[0]\n",
    "        predicted_word = id_to_word[most_likely_word_id]\n",
    "\n",
    "        if predicted_word == '<END>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(most_likely_word_id)\n",
    "\n",
    "    decoded_sequence = [id_to_word[word_id] for word_id in input_seq]\n",
    "\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_decoder():\n",
    "    input_seq = [word_to_id['<START>']]\n",
    "    while len(input_seq) < MAX_LEN:\n",
    "        padded_input_seq = pad_sequences([input_seq], maxlen=MAX_LEN, padding='pre')\n",
    "        predicted_word_probs = model.predict(padded_input_seq)\n",
    "        epsilon = 1e-7\n",
    "        normalized_word_probs = predicted_word_probs[0] / (np.sum(predicted_word_probs[0]+epsilon))\n",
    "        sampled_word_id = np.random.multinomial(1, normalized_word_probs.astype('float64')).argmax()\n",
    "        predicted_word = id_to_word[sampled_word_id]\n",
    "        if predicted_word == '<END>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(sampled_word_id)\n",
    "\n",
    "    decoded_sequence = [id_to_word[word_id] for word_id in input_seq]\n",
    "\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): \n",
    "    print(sample_decoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM=300\n",
    "IMAGE_ENC_DIM=300\n",
    "\n",
    "# Image input\n",
    "img_input = Input(shape=(2048,))\n",
    "img_enc = Dense(300, activation=\"relu\") (img_input)\n",
    "images = RepeatVector(MAX_LEN)(img_enc)\n",
    "\n",
    "# Text input\n",
    "text_input = Input(shape=(MAX_LEN,))\n",
    "embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\n",
    "x = Concatenate()([images,embedding])\n",
    "y = Bidirectional(LSTM(256, return_sequences=False))(x) \n",
    "pred = Dense(vocab_size, activation='softmax')(y)\n",
    "model = Model(inputs=[img_input,text_input],outputs=pred)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"RMSProp\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train = np.load(f\"{OUTPUT_PATH}/encoded_images_train.npy\")\n",
    "enc_dev = np.load(f\"{OUTPUT_PATH}/encoded_images_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(batch_size=128):\n",
    "    while True:\n",
    "        batch_image_ids = np.random.choice(train_list, batch_size)\n",
    "\n",
    "        image_inputs = []\n",
    "        text_inputs = []\n",
    "        next_words = []\n",
    "\n",
    "        for image_id in batch_image_ids:\n",
    "            descs = descriptions[image_id]\n",
    "            desc = random.choice(descs)\n",
    "\n",
    "            tokenized_desc = [word_to_id.get(word, None) for word in desc]\n",
    "            tokenized_desc = [word_id for word_id in tokenized_desc if word_id is not None]\n",
    "\n",
    "            for i in range(1, len(tokenized_desc)):\n",
    "                input_seq = tokenized_desc[:i]\n",
    "                output_word = tokenized_desc[i]\n",
    "\n",
    "                image_inputs.append(enc_train[train_list.index(image_id)])\n",
    "\n",
    "                text_inputs.append(input_seq)\n",
    "                next_words.append(output_word)\n",
    "\n",
    "\n",
    "        text_inputs = pad_sequences(text_inputs, maxlen=MAX_LEN, padding='pre')\n",
    "        next_words = to_categorical(next_words, num_classes=vocab_size)\n",
    "\n",
    "        yield ([np.array(image_inputs), np.array(text_inputs)], np.array(next_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "generator = training_generator(batch_size)\n",
    "steps = len(train_list) * MAX_LEN // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f\"{OUTPUT_PATH}/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_decoder(enc_image):\n",
    "    input_seq = [word_to_id['<START>']]\n",
    "\n",
    "    while len(input_seq) < MAX_LEN:\n",
    "        padded_input_seq = pad_sequences([input_seq], maxlen=MAX_LEN, padding='pre')\n",
    "\n",
    "        input_data = [enc_image.reshape(1, -1), padded_input_seq]\n",
    "\n",
    "        predicted_word_probs = model.predict(input_data, verbose=0)\n",
    "        sampled_word_id = int(np.argmax(predicted_word_probs[0]))\n",
    "        predicted_word = id_to_word[sampled_word_id]\n",
    "        if predicted_word == '<END>':\n",
    "            break\n",
    "\n",
    "        input_seq.append(sampled_word_id)\n",
    "\n",
    "    decoded_sequence = [id_to_word[word_id] for word_id in input_seq]\n",
    "    return decoded_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(train_list[0]))\n",
    "image_decoder(enc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(get_image(dev_list[1]))\n",
    "image_decoder(enc_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_beam_decoder(n, image_enc):\n",
    "    start_token = word_to_id['<START>']\n",
    "    end_token = word_to_id['<END>']\n",
    "\n",
    "    initial_beam = (1.0, [start_token])\n",
    "\n",
    "    beam_queue = PriorityQueue()\n",
    "    beam_queue.put((-initial_beam[0], initial_beam[1]))\n",
    "\n",
    "    for _ in range(MAX_LEN):\n",
    "        candidates = []\n",
    "\n",
    "        while not beam_queue.empty():\n",
    "            prob, seq = beam_queue.get()\n",
    "            prob = -prob  \n",
    "\n",
    "            padded_input_seq = pad_sequences([seq], maxlen=MAX_LEN, padding='pre')\n",
    "            input_data = [image_enc.reshape(1, -1), padded_input_seq]\n",
    "\n",
    "            predicted_word_probs = model.predict(input_data,verbose = 0).flatten()\n",
    "\n",
    "            top_word_indices = np.argpartition(predicted_word_probs, -n)[-n:]\n",
    "\n",
    "            for word_index in top_word_indices:\n",
    "                if word_index != end_token:\n",
    "                    new_prob = prob * predicted_word_probs[word_index]\n",
    "                    new_seq = seq + [word_index]\n",
    "                    candidates.append((new_prob, new_seq))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "        candidates = candidates[:n]\n",
    "\n",
    "        beam_queue = PriorityQueue()\n",
    "        for prob, seq in candidates:\n",
    "            beam_queue.put((-prob, seq))\n",
    "\n",
    "    best_sequence_prob, best_sequence = candidates[0]\n",
    "\n",
    "    decoded_sequence = [id_to_word[word_id] for word_id in best_sequence]\n",
    "\n",
    "    return decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_beam_decoder(3, enc_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(len(enc_dev), 5)\n",
    "images_to_show = [enc_dev[i] for i in indices]\n",
    "for image in images_to_show:\n",
    "    print(image_decoder(image))\n",
    "    print(img_beam_decoder(3,image))\n",
    "    print(img_beam_decoder(5, image))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
